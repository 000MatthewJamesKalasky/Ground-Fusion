# Ground-Fusion 

codes and datasets will be made public upon paper acceptance!

## Project Author: [Jie Yin](https://github.com/sjtuyinjie?tab=repositories) at 1195391308@qq.com 


## ABSTRACT:

We present Ground-Fusion: a multi-sensor SLAM system for ground vehicles featuring adaptive sensor selection, real-time dense color mapping, and robust localization with high accuracy. Our system tightly couples RGB-D images, inertial information, and wheel odometer measurements within a factor graph, and adopt an adaptive sensor selection strategy for diverse corner cases such as severe visual occlusion, wheel drift, and even wheel suspension. Experiments on both public and self-collected datasets demonstrate that our system shows competitive performance in accuracy and robustness comparable to LiDAR SLAM. For the benefit of the research community, the codes and datasets will be released at this website upon paper publication.

## MAIN CONTRIBUTIONS :

* We construct the first-ever optimization-based SLAM framework which tightly couples RGB-D images, inertial measurements and wheel odometer. Besides being capable of real-time tracking and dense mapping, the system supports line feature tracking and dynamic object detection as well.

 * An adaptive sensor selection strategy is adopted to strengthen the robustness and accuracy of the system in various corner cases. 

 * We collect a novel multi-sensor SLAM dataset with rich sensory measurements in diverse scenarios, which will be made public upon paper publication.


